{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 데이터 로더 준비\n",
    "- 학습용 / 테스트용 함수\n",
    "- 모델 클래스\n",
    "- 학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH_SIZE, LOSS_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv로 만든 파일 불러오기 (인코딩한 가사, 어휘사전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 1696), (7954, 1))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv로 만든 파일 불러오기.\n",
    "df = pd.read_csv('encoded.csv', index_col=0) # 0번째 열을 인덱스로 지정.\n",
    "vocaDF = pd.read_csv('vocab.csv', index_col=0)\n",
    "df.shape, vocaDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 라벨 분리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 1695), (760,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelSR = df.genre\n",
    "\n",
    "featureDF = df.iloc[:,:-1]\n",
    "\n",
    "featureDF.shape, labelSR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrn, Xtst, ytrn, ytst = train_test_split(featureDF, labelSR, test_size=0.2, random_state = 10, shuffle = True, stratify = labelSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "# 바로 변환이 안돼서 넘파이로 변환 후 다시 텐서로 변환.\n",
    "XtrnNP = np.array(Xtrn)\n",
    "XtstNP = np.array(Xtst)\n",
    "ytrnNP = np.array(ytrn)\n",
    "ytstNP = np.array(ytst)\n",
    "\n",
    "Xtrain = torch.LongTensor(XtrnNP)\n",
    "Xtest = torch.LongTensor(XtstNP)\n",
    "ytrain = torch.LongTensor(ytrnNP)\n",
    "ytest = torch.LongTensor(ytstNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain = torch.Size([608, 1695]), 2\n",
      "Xtest = torch.Size([152, 1695]), 2\n",
      "ytrain = torch.Size([608]), 1\n",
      "ytest = torch.Size([152]), 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Xtrain = {Xtrain.shape}, {Xtrain.ndim}')\n",
    "print(f'Xtest = {Xtest.shape}, {Xtest.ndim}')\n",
    "print(f'ytrain = {ytrain.shape}, {ytrain.ndim}')\n",
    "print(f'ytest = {ytest.shape}, {ytest.ndim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋, 로더 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "trainDS = TensorDataset(Xtrain, ytrain)\n",
    "testDS = TensorDataset(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle = True)\n",
    "testDL = DataLoader(testDS, BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, num_layers, dropout, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    # fc 레이어의 가중치(weight)와 편향(bias)을 초기화하는 메서드\n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.5\n",
    "        # uniform_으로 fc 레이어의 가중치를 -initrange에서 initrange 사이의 균등 분포(uniform distribution)를 가지도록 초기화\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange) \n",
    "        # zero_ 메서드를 사용하여 편향을 모두 0으로 초기화\n",
    "        self.fc.bias.data.zero_() \n",
    "\n",
    "    # forward 메서드 : 모델의 순전파(forward pass)\n",
    "    def forward(self, text): # text : 입력 받은 텍스트 데이터\n",
    "        embedded = self.embedding(text) # 입력 데이터를 임베딩\n",
    "        out, _ = self.lstm(embedded) # 임베딩된 데이터를 lstm 레이어에 입력\n",
    "        out = self.fc(out[:, -1, :]) # out[:, -1, :]를 사용하여 시퀀스의 마지막 시간 단계의 출력을 선택\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentenceClassifier(nn.Module) :\n",
    "#     def __init__(self, n_vocab, hidden_dim, embedding_dim, n_layers, dropout=0.5, bidirectional = True) :\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.embedding=nn.Embedding(num_embeddings =n_vocab,\n",
    "#                                     embedding_dim = embedding_dim,\n",
    "#                                     padding_idx =0)\n",
    "#         self.model = nn.LSTM(input_size = embedding_dim,\n",
    "#                              hidden_size=hidden_dim,\n",
    "#                              num_layers= n_layers,\n",
    "#                              bidirectional = bidirectional,\n",
    "#                              dropout = dropout,\n",
    "#                              batch_first = True)\n",
    "#         if bidirectional :\n",
    "#             self.classifier = nn.Linear(hidden_dim*2, 1)\n",
    "#         else :\n",
    "#             self.classifier = nn.Linear(hidden_dim, 1)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, inputs) :\n",
    "#         embeddings = self.embedding(inputs)\n",
    "#         output, _ = self.model(embeddings)\n",
    "#         last_out = output[:, -1, :]\n",
    "#         last_out = self.dropout(last_out)\n",
    "#         logits = self.classifier(last_out)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 64\n",
    "EMBEDD_DIM  = 128\n",
    "VOCAB_SIZE  = vocaDF.shape[0]\n",
    "NUM_LAYERS = 1\n",
    "EPOCHS      = 100\n",
    "LR          = 0.1\n",
    "BATCH_SIZE  = 1\n",
    "DROPOUT     = 0.5\n",
    "OUTPUT      = labelSR.nunique() # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\NLP_38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "mdl = LSTM(VOCAB_SIZE, EMBEDD_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, OUTPUT)\n",
    "#mdl = SentenceClassifier(VOCAB_SIZE, HIDDEN_SIZE, EMBEDD_DIM,NUM_LAYERS).to(device)\n",
    "# CRITERION = nn.BCEWithLogitsLoss().to(device)\n",
    "# OPTIMIZER = optim.RMSprop(mdl.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.AdamW(mdl.parameters(), lr=LR)\n",
    "SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # 학습 평가 관련 변수들\n",
    "    total_acc, total_count = 0,0 # 배치 학습 과정에서 정확도(accuracy)를 계산하기 위한 변수\n",
    "    log_interval=10 # 로그 출력 간격\n",
    "    \n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        #text, label = text.to(device), label.to(device) => GPU로 이동\n",
    "        predicted_label = model(text) # 모델에 입력 데이터를 전달하여 예측값을 계산\n",
    "        optimizer.zero_grad() # 기울기(gradient) 초기화\n",
    "\n",
    "        \n",
    "        loss = criterion(predicted_label, label) # 손실(loss) 계산\n",
    "        loss.backward() # 역전파 수행\n",
    "        \n",
    "        #함수는 그래디언트(기울기)의 노름(norm)을 클리핑(clipping)하는 유틸리티 함수\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) \n",
    "        \n",
    "        # 모델 파라미터 업데이트\n",
    "        optimizer.step() \n",
    "        \n",
    "        # 배치 학습 평가\n",
    "        # predicted_label.argmax(1) : 모델이 예측한 각 샘플의 클래스를 나타내는 텐서. argmax(1)은 각 행(row)에서 최대 값의 인덱스(클래스)를 가져옴.\n",
    "        # (predicted_label.argmax(1) == label) : 예측값과 실제 레이블(label)을 비교하여 일치하는지 여부를 판단하는 불리언(Boolean) 텐서를 생성\n",
    "        # .sum().item() : 불리언 텐서의 원소들을 합하여 정확하게 예측된 샘플 수를 계산\n",
    "        # 샘플 수를 total_acc에 누적\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item() \n",
    "        \n",
    "        # 전체 샘플 수 계산 - 레이블(label) 텐서의 첫 번째 차원의 크기를 반환. 배치의 샘플 수.\n",
    "        # => 현재 배치의 샘플 수를 누적\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        # log_interval 배치마다 로그를 출력하기 위한 조건\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            # 현재 학습 에포크(epoch)와 배치 인덱스(idx), 손실(loss) 값을 출력\n",
    "            print(f\"epoch : {epoch} batch : {idx} loss : {loss.item()}\")\n",
    "            \n",
    "            # 현재 배치의 정확도(accuracy)를 계산하여 출력\n",
    "            print(f\"Accuracy : {total_acc/total_count}\")\n",
    "            \n",
    "            # 정확하게 예측된 샘플 수(total_acc)와 전체 샘플 수를 다음 로그 출력을 위해 초기화\n",
    "            total_acc, total_count = 0,0\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    # 평가 모드\n",
    "    model.eval()\n",
    "    \n",
    "    total_acc, total_count = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            \n",
    "    return total_acc/total_count, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, text, text_pipeline):\n",
    "#     with torch.no_grad():\n",
    "#         text = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "#         text = text.unsqueeze(0)\n",
    "#         offsets = torch.tensor([0]).to(device)\n",
    "#         predicted_label = model(text, offsets)\n",
    "#         return predicted_label.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 1 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 2 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 2 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 3 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 3 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 4 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 4 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 5 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 5 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 6 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 6 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 7 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 7 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 8 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 8 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 9 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 9 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 10 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 10 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 11 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 11 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 12 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 12 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 13 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 13 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 14 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 14 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 15 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 15 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 16 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 16 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 17 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 17 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 18 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.36363636363636365\n",
      "test - [epoch] : 18 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 19 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 19 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 20 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 20 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 21 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 21 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 22 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 22 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 23 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 23 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 24 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 24 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 25 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 25 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 26 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 26 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 27 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 27 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 28 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 28 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 29 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 29 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 30 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 30 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 31 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 31 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 32 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 32 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 33 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 33 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 34 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 34 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 35 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 35 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 36 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 36 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 37 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 37 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 38 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 38 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 39 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 39 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 40 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 40 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 41 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 41 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 42 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 42 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 43 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 43 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 44 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 44 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 45 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 45 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 46 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 46 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 47 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 47 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 48 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 48 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 49 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 49 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 50 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 50 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 51 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 51 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 52 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 52 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 53 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 53 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 54 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 54 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 55 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 55 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 56 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 56 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 57 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 57 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 58 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 58 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 59 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 59 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 60 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 60 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 61 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 61 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 62 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 62 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 63 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 63 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 64 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 64 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 65 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 65 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 66 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 66 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 67 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 67 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 68 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 68 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 69 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 69 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 70 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 70 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 71 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 71 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 72 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 72 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 73 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 73 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 74 batch : 10 loss : 1.8449842929840088\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 74 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 75 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 75 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 76 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 76 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 77 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 77 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 78 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 78 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 79 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 79 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 80 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 80 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 81 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 81 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 82 batch : 10 loss : 2.205381393432617\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 82 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 83 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 83 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 84 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 84 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 85 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 85 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 86 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 86 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 87 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 87 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 88 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 88 [Accuracy] : 0.13157894736842105  [loss] : 2.179597854614258\n",
      "\n",
      "epoch : 89 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 89 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 90 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 90 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n",
      "epoch : 91 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 91 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 92 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.2727272727272727\n",
      "test - [epoch] : 92 [Accuracy] : 0.13157894736842105  [loss] : 1.9659347534179688\n",
      "\n",
      "epoch : 93 batch : 10 loss : 2.351562023162842\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 93 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 94 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 94 [Accuracy] : 0.13157894736842105  [loss] : 2.351562023162842\n",
      "\n",
      "epoch : 95 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 95 [Accuracy] : 0.13157894736842105  [loss] : 1.8449842929840088\n",
      "\n",
      "epoch : 96 batch : 10 loss : 1.9659347534179688\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 96 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 97 batch : 10 loss : 1.793044090270996\n",
      "Accuracy : 0.36363636363636365\n",
      "test - [epoch] : 97 [Accuracy] : 0.13157894736842105  [loss] : 2.29099178314209\n",
      "\n",
      "epoch : 98 batch : 10 loss : 2.1557817459106445\n",
      "Accuracy : 0.09090909090909091\n",
      "test - [epoch] : 98 [Accuracy] : 0.13157894736842105  [loss] : 2.205381393432617\n",
      "\n",
      "epoch : 99 batch : 10 loss : 2.179597854614258\n",
      "Accuracy : 0.18181818181818182\n",
      "test - [epoch] : 99 [Accuracy] : 0.13157894736842105  [loss] : 1.793044090270996\n",
      "\n",
      "epoch : 100 batch : 10 loss : 2.29099178314209\n",
      "Accuracy : 0.0\n",
      "test - [epoch] : 100 [Accuracy] : 0.13157894736842105  [loss] : 2.1557817459106445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(mdl, trainDL, OPTIMIZER, CRITERION, epoch)\n",
    "    accu_val, loss = evaluate(mdl, testDL, CRITERION)\n",
    "    print(f\"test - [epoch] : {epoch} [Accuracy] : {accu_val}  [loss] : {loss}\\n\")\n",
    "    SCHEDULER.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행\n",
    "# for epoch in range(1, EPOCHS+1):\n",
    "#     train(mdl, trainDL, OPTIMIZER, CRITERION, epoch)\n",
    "#     accu_val = evaluate(mdl, testDL, CRITERION)\n",
    "#     print(f\"epoch : {epoch} Accuracy : {accu_val}\")\n",
    "#     SCHEDULER.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(mdl, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
